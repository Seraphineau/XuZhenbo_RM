
@inproceedings{SimonyanZisserman2014,
  title={Very Deep Convolutional Networks for Large-Scale Image Recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  booktitle={Proceedings of the International Conference on Learning Representations},
  pages={1--14},
  year={2014},
  abstract={In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3×3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  keywords={Deep Learning, Image Recognition,Feature extraction and deep learning techniques, Convolutional Networks, Large-Scale Image Recognition},
  publisher={ICLR},
  series={ICLR},
  volume={22},
  number={01},
  doi={10.48550/arXiv.1409.1556},
  url={https://doi.org/10.48550/arXiv.1409.1556}
}

@inproceedings{He2016,
  title={Deep Residual Learning for Image Recognition},
  author={He, Kaiming and others},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={770--778},
  year={2016},
  abstract={Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  keywords={Deep Learning, Image Recognition,Feature extraction and deep learning techniques, Neural Networks, Residual Learning},
  publisher={IEEE},
  series={CVPR},
  volume={22},
  number={01},
  doi={10.1109/CVPR.2016.90},
  url={https://doi.org/10.1109/CVPR.2016.90}
}

@article{Ren2017,
  title={Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={39},
  number={6},
  pages={1137--1149},
  year={2017},
  abstract={State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  keywords={Object Detection, Region Proposal Networks,Two-stage object detection methods, Convolutional Neural Networks, Real-time Object Detection},
  publisher={IEEE},
  series={TPAMI},
  doi={10.1109/TPAMI.2016.2577031},
  url={https://doi.org/10.1109/TPAMI.2016.2577031}
}

@article{Girshick2014,
  title={Rich feature hierarchies for accurate object detection and semantic segmentation},
  author={Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  journal={Proceedings of the IEEE conference on computer vision and pattern recognition},
  year={2014},
  volume={},
  number={},
  abstract={Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012—achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. Source code for the complete system is available.},
  keywords={Object Detection, Semantic Segmentation,Feature extraction and deep learning techniques, CNN, R-CNN},
  publisher={IEEE},
  series={CVPR},
  url={https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.html}
}

@article{He2017,
  title={Mask R-CNN},
  author={He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
  journal={Proceedings of the IEEE international conference on computer vision},
  year={2017},
  volume={},
  number={},
  abstract={We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available.},
  keywords={Object Detection,Two-stage object detection methods, Instance Segmentation, Mask R-CNN, CNN},
  publisher={IEEE},
  series={ICCV},
  url={https://openaccess.thecvf.com/content_ICCV_2017/html/Kaiming_He_Mask_R-CNN_ICCV_2017_paper.html}
}

@inproceedings{lan2018,
  title={Defect Detection from UAV Images Based on Region-Based CNNs},
  author={Lan, Meng and Zhang, Yipeng and Zhang, Lefei and Du Bo},
  booktitle={2018 18th IEEE International Conference on Data Mining Workshops (ICDMW)},
  pages={385--390},
  year={2018},
  organization={IEEE},
  doi={10.1109/ICDMW.2018.00063},
  url={https://doi.org/10.1109/ICDMW.2018.00063},
  keywords={Object detection, CNN, Unmanned Aerial Vehicle,Feature extraction and deep learning techniques, Defect detection, Electrical equipment, Machine Learning, Image processing, Deep learning, Real-time detection},
  abstract={With the wide applications of Unmanned Aerial Vehicle (UAV) in engineering such as the inspection of the electrical equipment from distance, the demands of efficient object detection algorithms for abundant images acquired by UAV have also been significantly increased in recent years. In computer vision and data mining communities, traditional object detection methods usually train a class-specific learner (e.g., the SVM) based on the low level features to detect the single class of images by sliding a local window. Thus, they may not suit for the UAV images with complex background and multiple kinds of interest objects. Recently, the deep convolutional neural networks (CNNs) have already shown great advances in the object detection and segmentation fields and outperformed many traditional methods which usually been employed in the past decades. In this work, we study the performance of the regionbased CNN for the electrical equipment defect detection by using the UAV images. In order to train the detection model, we collect a UAV images dataset composes of four classes of electrical equipment defects with thousands of annotated labels. Then, based on the region-based faster R-CNN model, we present a multi-class defects detection model for electrical equipment which is more efficient and accurate than traditional single class detection methods. Technically, we have replaced the RoI pooling layer with a similar operation in Tensorflow and promoted the mini-batch to 128 per image in the training procedure. These improvements have slightly increased the speed of detection without any accuracy loss. Therefore, the modified region-based CNN could simultaneously detect multi-class of defects of the electrical devices in nearly real time. Experimental results on the real word electrical equipment images demonstrate that the proposed method achieves better performance than the traditional object detection algorithms in defect detection.}
}


@article{Liu2016,
  title={SSD: Single Shot MultiBox Detector},
  author={Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  journal={European conference on computer vision},
  year={2016},
  volume={},
  number={},
  abstract={We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For 300×300 input, SSD achieves 74.3 % mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512 × 512 input, SSD achieves 76.9 % mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available.},
  keywords={Object Detection, SSD,One-stage object detection methods, Single Shot MultiBox Detector, CNN},
  publisher={Springer},
  series={ECCV},
  url={https://link.springer.com/chapter/10.1007/978-3-319-46448-0_2}
}
@article{Redmon2016,
  title={You Only Look Once: Unified, Real-Time Object Detection},
  author={Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  journal={Proceedings of the IEEE conference on computer vision and pattern recognition},
  year={2016},
  volume={},
  number={},
  abstract={We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.},
  keywords={Object Detection,One-stage object detection methods, YOLO, Real-Time Detection, CNN},
  publisher={IEEE},
  url={https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Redmon_You_Only_Look_CVPR_2016_paper.html}
}

@article{Uijlings2013,
  title={Selective Search for Object Recognition},
  author={Uijlings, Jasper R.R. and van de Sande, Koen E.A. and Gevers, Theo and Smeulders, Arnold W.M.},
  journal={International Journal of Computer Vision},
  year={2013},
  volume={104},
  number={2},
  abstract={This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99% recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available.},
  keywords={Object Recognition,Two-stage object detection methods, Selective Search, Object Locations},
  publisher={Springer},
  url={https://link.springer.com/article/10.1007/s11263-013-0620-5}
}

@article{Zhang2018,
  title={Single-Shot Refinement Neural Network for Object Detection},
  author={Zhang, Shifeng and Chi, Cheng and Yao, Yongqiang and Lei, Zhen and Li, Stan Z.},
  journal={Proceedings of the IEEE conference on computer vision and pattern recognition},
  year={2018},
  volume={},
  number={},
  abstract={For object detection, the two-stage approach (e.g., Faster R-CNN) has been achieving the highest accuracy, whereas the one-stage approach (e.g., SSD) has the advantage of high efficiency. To inherit the merits of both while overcoming their disadvantages, in this paper, we propose a novel single-shot based detector, called RefineDet, that achieves better accuracy than two-stage methods and maintains comparable efficiency of one-stage methods. RefineDet consists of two inter-connected modules, namely, the anchor refinement module and the object detection module. Specifically, the former aims to (1) filter out negative anchors to reduce search space for the classifier, and (2) coarsely adjust the locations and sizes of anchors to provide better initialization for the subsequent regressor. The latter module takes the refined anchors as the input from the former to further improve the regression accuracy and predict multi-class label. Meanwhile, we design a transfer connection block to transfer the features in the anchor refinement module to predict locations, sizes and class labels of objects in the object detection module. The multitask loss function enables us to train the whole network in an end-to-end way. Extensive experiments on PASCAL VOC 2007, PASCAL VOC 2012, and MS COCO demonstrate that RefineDet achieves state-of-the-art detection accuracy with high efficiency. Code is available at https://github.com/sfzhang15/RefineDet.},
  keywords={Object Detection, RefineDet,One-stage object detection methods, Anchor Refinement, CNN},
  publisher={IEEE},
  url={https://www.cv-foundation.org/openaccess/content_cvpr_2018/html/Zhang_Single-Shot_Refinement_Neural_CVPR_2018_paper.html}
}
